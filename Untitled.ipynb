{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bd2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11846336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data',\n",
    "                                 train = True,\n",
    "                                 transform = ToTensor(),\n",
    "                                 download = True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = './data',\n",
    "                                train = False,\n",
    "                                transform = ToTensor(),\n",
    "                                download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9faeaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fdea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f545515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super(ConvNeuralNet, self).__init__()\n",
    "    self.conv_l1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=2)\n",
    "    self.avg_pool1 = nn.AvgPool2d(kernel_size=4,stride=3)\n",
    "    \n",
    "    self.conv_l2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2)\n",
    "    self.avg_pool2 = nn.AvgPool2d(kernel_size=4, stride=3)\n",
    "    self.fc1 = nn.Linear(128,256)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(256,num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv_l1(x)\n",
    "    out = self.avg_pool1(out)\n",
    "    out = self.conv_l2(out)\n",
    "    \n",
    "    out = self.avg_pool2(out)\n",
    "\n",
    "    out = out.reshape(out.size(0), -1)\n",
    "\n",
    "    out = self.fc1(out)\n",
    "    out = self.relu1(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab5e69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2850fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eeda08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], loss:2.0198\n",
      "accuracy =  20.8\n",
      "\n",
      "\n",
      "\n",
      "Epoch [2], loss:2.0367\n",
      "accuracy =  23.849999999999998\n",
      "\n",
      "\n",
      "\n",
      "Epoch [3], loss:1.9850\n",
      "accuracy =  25.21\n",
      "\n",
      "\n",
      "\n",
      "Epoch [4], loss:2.0986\n",
      "accuracy =  26.119999999999997\n",
      "\n",
      "\n",
      "\n",
      "Epoch [5], loss:2.0255\n",
      "accuracy =  28.33\n",
      "\n",
      "\n",
      "\n",
      "Epoch [6], loss:2.0131\n",
      "accuracy =  29.32\n",
      "\n",
      "\n",
      "\n",
      "Epoch [7], loss:1.7932\n",
      "accuracy =  29.68\n",
      "\n",
      "\n",
      "\n",
      "Epoch [8], loss:1.9844\n",
      "accuracy =  31.97\n",
      "\n",
      "\n",
      "\n",
      "Epoch [9], loss:1.7952\n",
      "accuracy =  32.6\n",
      "\n",
      "\n",
      "\n",
      "Epoch [10], loss:1.8921\n",
      "accuracy =  32.92\n",
      "\n",
      "\n",
      "\n",
      "Epoch [11], loss:1.8075\n",
      "accuracy =  33.69\n",
      "\n",
      "\n",
      "\n",
      "Epoch [12], loss:1.8160\n",
      "accuracy =  33.76\n",
      "\n",
      "\n",
      "\n",
      "Epoch [13], loss:1.8573\n",
      "accuracy =  34.77\n",
      "\n",
      "\n",
      "\n",
      "Epoch [14], loss:1.6420\n",
      "accuracy =  35.02\n",
      "\n",
      "\n",
      "\n",
      "Epoch [15], loss:1.8031\n",
      "accuracy =  35.120000000000005\n",
      "\n",
      "\n",
      "\n",
      "Epoch [16], loss:1.9136\n",
      "accuracy =  35.76\n",
      "\n",
      "\n",
      "\n",
      "Epoch [17], loss:1.8075\n",
      "accuracy =  36.199999999999996\n",
      "\n",
      "\n",
      "\n",
      "Epoch [18], loss:1.7396\n",
      "accuracy =  36.57\n",
      "\n",
      "\n",
      "\n",
      "Epoch [19], loss:1.8302\n",
      "accuracy =  36.54\n",
      "\n",
      "\n",
      "\n",
      "Epoch [20], loss:1.8227\n",
      "accuracy =  36.620000000000005\n",
      "\n",
      "\n",
      "\n",
      "Epoch [21], loss:1.8757\n",
      "accuracy =  37.13\n",
      "\n",
      "\n",
      "\n",
      "Epoch [22], loss:1.6193\n",
      "accuracy =  37.47\n",
      "\n",
      "\n",
      "\n",
      "Epoch [23], loss:1.8482\n",
      "accuracy =  37.29\n",
      "\n",
      "\n",
      "\n",
      "Epoch [24], loss:1.6751\n",
      "accuracy =  37.43\n",
      "\n",
      "\n",
      "\n",
      "Epoch [25], loss:1.8276\n",
      "accuracy =  37.269999999999996\n",
      "\n",
      "\n",
      "\n",
      "Epoch [26], loss:1.7353\n",
      "accuracy =  38.0\n",
      "\n",
      "\n",
      "\n",
      "Epoch [27], loss:1.6806\n",
      "accuracy =  37.97\n",
      "\n",
      "\n",
      "\n",
      "Epoch [28], loss:1.6934\n",
      "accuracy =  38.39\n",
      "\n",
      "\n",
      "\n",
      "Epoch [29], loss:1.8216\n",
      "accuracy =  38.190000000000005\n",
      "\n",
      "\n",
      "\n",
      "Epoch [30], loss:1.7692\n",
      "accuracy =  38.37\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs,labels)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Desktop\\CRISS\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[54], line 14\u001b[0m, in \u001b[0;36mConvNeuralNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_l1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool1(out)\n\u001b[0;32m     16\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_l2(out)\n",
      "File \u001b[1;32m~\\Desktop\\CRISS\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\CRISS\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\CRISS\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc=[0,0]\n",
    "k = 0\n",
    "j = 0\n",
    "while True:\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs,labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i/195==1:\n",
    "      j+=1\n",
    "      with torch.no_grad():\n",
    "        correct = 0\n",
    "        samples = 0\n",
    "        for images, labels in test_loader:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(images)\n",
    "          _,predicted = torch.max(outputs.data,1)\n",
    "          samples+=labels.size(0)\n",
    "          correct+= (predicted == labels).sum().item()\n",
    "      print('Epoch [{}], loss:{:.4f}'.format(j,loss.item()))\n",
    "      print('accuracy = ',100*(correct/samples))\n",
    "      print('\\n\\n')\n",
    "      accuracy = correct/samples\n",
    "      acc+=[accuracy]\n",
    "      if acc[len(acc)-1]<acc[len(acc)-2] and acc[len(acc)-2]<acc[len(acc)-3]:\n",
    "        k+=1\n",
    "        break\n",
    "  if k==1:\n",
    "    break\n",
    "\n",
    "plt.plot(acc[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e049d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model2.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47cd28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNeuralNet(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"model2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8326ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  38.54\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        correct = 0\n",
    "        samples = 0\n",
    "        for images, labels in test_loader:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(images)\n",
    "          _,predicted = torch.max(outputs.data,1)\n",
    "          samples+=labels.size(0)\n",
    "          correct+= (predicted == labels).sum().item()\n",
    "print('accuracy = ',100*(correct/samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b881954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
